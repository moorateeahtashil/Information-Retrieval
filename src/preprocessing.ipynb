{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1 (Updated): Preprocessing the Dataset\n",
    "\n",
    "This notebook covers **Step 1** of the information retrieval project using the `.jsonl` and `.tsv` file formats. We will load the data, apply a series of preprocessing steps, and save the cleaned data for the retrieval models.\n",
    "\n",
    "**Preprocessing Pipeline:**\n",
    "1.  **Load Data**: Read `corpus.jsonl`, `queries.jsonl`, and the `qrels/*.tsv` files.\n",
    "2.  **Tokenization**: Split text into individual words (tokens).\n",
    "3.  **Lowercasing**: Convert all text to lowercase.\n",
    "4.  **Stopword & Punctuation Removal**: Remove common English words and punctuation.\n",
    "5.  **POS Tagging & Lemmatization**: Reduce words to their base form (lemma) using part-of-speech context.\n",
    "6.  **Save Processed Data**: Store the cleaned data for use in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, we'll install and import the necessary libraries. We'll use `pandas` for data handling and `nltk` for natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/moorateeahtashil/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: nltk in /Users/moorateeahtashil/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/moorateeahtashil/Library/Python/3.9/lib/python/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/moorateeahtashil/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/moorateeahtashil/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/moorateeahtashil/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: click in /Users/moorateeahtashil/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/moorateeahtashil/Library/Python/3.9/lib/python/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/moorateeahtashil/Library/Python/3.9/lib/python/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/moorateeahtashil/Library/Python/3.9/lib/python/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "\n",
      "✅ NLTK resources downloaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/moorateeahtashil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/moorateeahtashil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/moorateeahtashil/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/moorateeahtashil/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/moorateeahtashil/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/moorateeahtashil/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas nltk\n",
    "\n",
    "# Download the necessary NLTK data packages\n",
    "import nltk\n",
    "nltk.download('punkt')                      # tokenizer\n",
    "nltk.download('stopwords')                  # stopword list\n",
    "nltk.download('wordnet')                    # lemmatizer lexicon\n",
    "nltk.download('omw-1.4')                    # multilingual WordNet data (lemmatizer often needs it)\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')\n",
    "print(\"\\n✅ NLTK resources downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Dataset\n",
    "\n",
    "This section is updated to handle the new file formats:\n",
    "- **`corpus.jsonl` & `queries.jsonl`**: These are JSON Lines files, where each line is a separate JSON object. We read them line-by-line.\n",
    "- **`qrels/*.tsv`**: These are Tab-Separated Value files. We'll load them and combine them into a single DataFrame for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus from ../fiqa/corpus.jsonl...\n",
      "Loaded 57638 documents.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>I'm not saying I don't like the idea of on-the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>So nothing preventing false ratings besides ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56</td>\n",
       "      <td>You can never use a health FSA for individual ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59</td>\n",
       "      <td>Samsung created the LCD and other flat screen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63</td>\n",
       "      <td>Here are the SEC requirements: The federal sec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_id                                               text\n",
       "0      3  I'm not saying I don't like the idea of on-the...\n",
       "1     31  So nothing preventing false ratings besides ad...\n",
       "2     56  You can never use a health FSA for individual ...\n",
       "3     59  Samsung created the LCD and other flat screen ...\n",
       "4     63  Here are the SEC requirements: The federal sec..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading queries from ../fiqa/queries.jsonl...\n",
      "Loaded 6648 queries.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What is considered a business expense on a bus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Business Expense - Car Insurance Deductible Fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Starting a new online business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>“Business day” and “due date” for bills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>New business owner - How do taxes work for the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_id                                               text\n",
       "0        0  What is considered a business expense on a bus...\n",
       "1        4  Business Expense - Car Insurance Deductible Fo...\n",
       "2        5                     Starting a new online business\n",
       "3        6            “Business day” and “due date” for bills\n",
       "4        7  New business owner - How do taxes work for the..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading qrels from ../fiqa/qrels/...\n",
      "Loaded and combined 17110 relevance judgments from 3 files.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18850</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>196463</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>69306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>560251</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>188530</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_id  doc_id  score\n",
       "0        0   18850      1\n",
       "1        4  196463      1\n",
       "2        5   69306      1\n",
       "3        6  560251      1\n",
       "4        6  188530      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Adjust these paths if your files are in a different location.\n",
    "CORPUS_FILE = '../fiqa/corpus.jsonl'\n",
    "QUERIES_FILE = '../fiqa/queries.jsonl'\n",
    "QRELS_DIR = '../fiqa/qrels/'\n",
    "\n",
    "# --- Load Corpus --- \n",
    "print(f\"Loading corpus from {CORPUS_FILE}...\")\n",
    "corpus_data = []\n",
    "with open(CORPUS_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        # Best practice: combine title and text for a richer document representation\n",
    "        full_text = item.get('title', '') + ' ' + item.get('text', '')\n",
    "        corpus_data.append({\n",
    "            'doc_id': str(item['_id']), # Ensure IDs are strings\n",
    "            'text': full_text.strip()\n",
    "        })\n",
    "corpus_df = pd.DataFrame(corpus_data)\n",
    "print(f\"Loaded {len(corpus_df)} documents.\")\n",
    "display(corpus_df.head())\n",
    "\n",
    "# --- Load Queries ---\n",
    "print(f\"\\nLoading queries from {QUERIES_FILE}...\")\n",
    "queries_data = []\n",
    "with open(QUERIES_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        queries_data.append({\n",
    "            'query_id': str(item['_id']), # Ensure IDs are strings\n",
    "            'text': item['text']\n",
    "        })\n",
    "queries_df = pd.DataFrame(queries_data)\n",
    "print(f\"Loaded {len(queries_df)} queries.\")\n",
    "display(queries_df.head())\n",
    "\n",
    "# --- Load and Combine Qrels ---\n",
    "print(f\"\\nLoading qrels from {QRELS_DIR}...\")\n",
    "qrels_files = glob(os.path.join(QRELS_DIR, '*.tsv'))\n",
    "qrels_df_list = []\n",
    "for file_path in qrels_files:\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    # Standardize column names\n",
    "    df.rename(columns={'query-id': 'query_id', 'corpus-id': 'doc_id'}, inplace=True)\n",
    "    # Ensure IDs are strings for consistency\n",
    "    df['query_id'] = df['query_id'].astype(str)\n",
    "    df['doc_id'] = df['doc_id'].astype(str)\n",
    "    qrels_df_list.append(df)\n",
    "\n",
    "qrels_df = pd.concat(qrels_df_list, ignore_index=True)\n",
    "# Filter for only positive relevance scores\n",
    "qrels_df = qrels_df[qrels_df['score'] > 0]\n",
    "print(f\"Loaded and combined {len(qrels_df)} relevance judgments from {len(qrels_files)} files.\")\n",
    "display(qrels_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Pipeline\n",
    "\n",
    "This text processing function is the same as before. It tokenizes, cleans, and lemmatizes any given text, making it perfect for both documents and queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Investing in stocks can be a rewarding but risky endeavor. What are the best strategies for beginners?\n",
      "Processed: ['invest', 'stock', 'rewarding', 'risky', 'endeavor', 'best', 'strategy', 'beginner']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Map NLTK POS tag to a format WordNetLemmatizer can understand.\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN # Default to noun\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Applies the full preprocessing pipeline to a single string of text.\"\"\"\n",
    "    # 1. Tokenize and lowercase\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # 2. Part-of-Speech (POS) tagging\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # 3. Lemmatize with POS tags and remove stopwords/punctuation\n",
    "    lemmas = []\n",
    "    for word, tag in pos_tags:\n",
    "        # Check if it's a stopword or just punctuation\n",
    "        if word not in stop_words and word not in punctuation and word.isalpha():\n",
    "            # Get the correct POS tag for the lemmatizer\n",
    "            wnet_pos = get_wordnet_pos(tag)\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wnet_pos)\n",
    "            lemmas.append(lemma)\n",
    "            \n",
    "    return lemmas\n",
    "\n",
    "# --- Example of preprocessing ---\n",
    "sample_text = \"Investing in stocks can be a rewarding but risky endeavor. What are the best strategies for beginners?\"\n",
    "processed_sample = preprocess_text(sample_text)\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Processed: {processed_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply Preprocessing to Corpus and Queries\n",
    "\n",
    "Now, we apply our function to the `text` column of both the corpus and queries DataFrames. This step can take some time on large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57638/57638 [03:15<00:00, 294.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6648/6648 [00:02<00:00, 3011.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processed Corpus Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>I'm not saying I don't like the idea of on-the...</td>\n",
       "      <td>[say, like, idea, training, ca, expect, compan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>So nothing preventing false ratings besides ad...</td>\n",
       "      <td>[nothing, prevent, false, rating, besides, add...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56</td>\n",
       "      <td>You can never use a health FSA for individual ...</td>\n",
       "      <td>[never, use, health, fsa, individual, health, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59</td>\n",
       "      <td>Samsung created the LCD and other flat screen ...</td>\n",
       "      <td>[samsung, create, lcd, flat, screen, technolog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63</td>\n",
       "      <td>Here are the SEC requirements: The federal sec...</td>\n",
       "      <td>[sec, requirement, federal, security, law, def...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_id                                               text  \\\n",
       "0      3  I'm not saying I don't like the idea of on-the...   \n",
       "1     31  So nothing preventing false ratings besides ad...   \n",
       "2     56  You can never use a health FSA for individual ...   \n",
       "3     59  Samsung created the LCD and other flat screen ...   \n",
       "4     63  Here are the SEC requirements: The federal sec...   \n",
       "\n",
       "                                      processed_text  \n",
       "0  [say, like, idea, training, ca, expect, compan...  \n",
       "1  [nothing, prevent, false, rating, besides, add...  \n",
       "2  [never, use, health, fsa, individual, health, ...  \n",
       "3  [samsung, create, lcd, flat, screen, technolog...  \n",
       "4  [sec, requirement, federal, security, law, def...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processed Queries Sample ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What is considered a business expense on a bus...</td>\n",
       "      <td>[consider, business, expense, business, trip]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Business Expense - Car Insurance Deductible Fo...</td>\n",
       "      <td>[business, expense, car, insurance, deductible...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Starting a new online business</td>\n",
       "      <td>[start, new, online, business]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>“Business day” and “due date” for bills</td>\n",
       "      <td>[business, day, due, date, bill]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>New business owner - How do taxes work for the...</td>\n",
       "      <td>[new, business, owner, tax, work, business, v,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_id                                               text  \\\n",
       "0        0  What is considered a business expense on a bus...   \n",
       "1        4  Business Expense - Car Insurance Deductible Fo...   \n",
       "2        5                     Starting a new online business   \n",
       "3        6            “Business day” and “due date” for bills   \n",
       "4        7  New business owner - How do taxes work for the...   \n",
       "\n",
       "                                      processed_text  \n",
       "0      [consider, business, expense, business, trip]  \n",
       "1  [business, expense, car, insurance, deductible...  \n",
       "2                     [start, new, online, business]  \n",
       "3                   [business, day, due, date, bill]  \n",
       "4  [new, business, owner, tax, work, business, v,...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"Preprocessing corpus...\")\n",
    "# Using progress_apply to see a progress bar\n",
    "corpus_df['processed_text'] = corpus_df['text'].progress_apply(preprocess_text)\n",
    "\n",
    "print(\"\\nPreprocessing queries...\")\n",
    "queries_df['processed_text'] = queries_df['text'].progress_apply(preprocess_text)\n",
    "\n",
    "print(\"\\n--- Processed Corpus Sample ---\")\n",
    "display(corpus_df.head())\n",
    "\n",
    "print(\"\\n--- Processed Queries Sample ---\")\n",
    "display(queries_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create an Inverted Index\n",
    "\n",
    "An **inverted index** is a data structure that maps terms to the documents containing them. It is the cornerstone of efficient search. We build it from our processed corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Index: 100%|██████████| 57638/57638 [00:02<00:00, 26615.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documents containing the word 'stock':\n",
      "['260094', '368079', '57960', '98654', '539263', '434838', '305128', '513734', '217286', '354815']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def create_inverted_index(corpus_df):\n",
    "    inverted_index = defaultdict(set) # Use a set to avoid duplicate doc_ids\n",
    "    for _, row in tqdm(corpus_df.iterrows(), total=corpus_df.shape[0], desc=\"Building Index\"):\n",
    "        doc_id = row['doc_id']\n",
    "        terms = row['processed_text']\n",
    "        for term in terms:\n",
    "            inverted_index[term].add(doc_id)\n",
    "    # Convert sets to lists for JSON serialization\n",
    "    for term in inverted_index:\n",
    "        inverted_index[term] = list(inverted_index[term])\n",
    "    return inverted_index\n",
    "\n",
    "inverted_index = create_inverted_index(corpus_df[['doc_id', 'processed_text']])\n",
    "\n",
    "# --- Example lookup in the inverted index ---\n",
    "if 'stock' in inverted_index:\n",
    "    print(\"\\nDocuments containing the word 'stock':\")\n",
    "    print(inverted_index['stock'][:10]) # Show first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Processed Data\n",
    "\n",
    "Finally, we save our processed DataFrames and the inverted index to disk. We will use these files in the next notebook (`2_Models_and_Evaluation.ipynb`) to build and evaluate our IR models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Preprocessing complete. All processed data saved to the '../fiqa/processed_data' directory.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "OUTPUT_DIR = '../fiqa/processed_data'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# Save processed dataframes using pickle for efficiency\n",
    "corpus_df.to_pickle(os.path.join(OUTPUT_DIR, 'corpus_processed.pkl'))\n",
    "queries_df.to_pickle(os.path.join(OUTPUT_DIR, 'queries_processed.pkl'))\n",
    "qrels_df.to_pickle(os.path.join(OUTPUT_DIR, 'qrels.pkl'))\n",
    "\n",
    "# Save inverted index as a JSON file\n",
    "with open(os.path.join(OUTPUT_DIR, 'inverted_index.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(inverted_index, f)\n",
    "\n",
    "print(f\"\\n✅ Preprocessing complete. All processed data saved to the '{OUTPUT_DIR}' directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
