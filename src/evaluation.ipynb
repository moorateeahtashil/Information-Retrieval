{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comprehensive IR Model Evaluation\n",
        "\n",
        "This notebook provides a comprehensive evaluation of all Information Retrieval models implemented in the project:\n",
        "\n",
        "1. **Classical IR Models**:\n",
        "   - TF-IDF (Scikit-learn optimized)\n",
        "   - BM25 (Default parameters)\n",
        "   - BM25 (Optimized parameters)\n",
        "\n",
        "2. **Neural IR Models**:\n",
        "   - Bi-encoder + Cross-encoder pipeline\n",
        "\n",
        "## Evaluation Metrics\n",
        "\n",
        "### Performance Metrics:\n",
        "- **Precision@k**: Precision at different cutoff values\n",
        "- **Recall@k**: Recall at different cutoff values\n",
        "- **Mean Average Precision (MAP)**: Overall ranking quality\n",
        "- **Mean Reciprocal Rank (MRR)**: First relevant result position\n",
        "- **nDCG@k**: Normalized Discounted Cumulative Gain\n",
        "- **F1@k**: Harmonic mean of precision and recall\n",
        "\n",
        "### Efficiency Metrics:\n",
        "- **Query Latency**: Time to process individual queries\n",
        "- **Memory Usage**: RAM consumption during operation\n",
        "- **CPU Usage**: Processor utilization\n",
        "- **Throughput**: Queries processed per second\n",
        "- **Index Size**: Storage requirements for models\n",
        "- **Model Size**: Memory footprint of loaded models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import psutil\n",
        "import gc\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rank_bm25 import BM25Okapi\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "import faiss\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data and Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed data\n",
        "DATA_DIR = '../fiqa/processed_data'\n",
        "\n",
        "print(\"Loading processed data...\")\n",
        "corpus_df = pd.read_pickle(os.path.join(DATA_DIR, 'corpus_processed.pkl'))\n",
        "queries_df = pd.read_pickle(os.path.join(DATA_DIR, 'queries_processed.pkl'))\n",
        "qrels_df = pd.read_pickle(os.path.join(DATA_DIR, 'qrels.pkl'))\n",
        "\n",
        "# Create mappings for faster lookups\n",
        "query_id_to_text = pd.Series(queries_df.processed_text.values, index=queries_df.query_id).to_dict()\n",
        "query_id_to_original = pd.Series(queries_df.text.values, index=queries_df.query_id).to_dict()\n",
        "doc_id_to_text = pd.Series(corpus_df.text.values, index=corpus_df.doc_id).to_dict()\n",
        "\n",
        "# Create relevance mapping\n",
        "relevant_docs = qrels_df.groupby('query_id')['doc_id'].apply(list).to_dict()\n",
        "\n",
        "print(f\"Data loaded successfully:\")\n",
        "print(f\"  - Corpus: {len(corpus_df)} documents\")\n",
        "print(f\"  - Queries: {len(queries_df)} queries\")\n",
        "print(f\"  - Relevance judgments: {len(qrels_df)}\")\n",
        "print(f\"  - Queries with relevance: {len(relevant_docs)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Implementations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classical IR Models\n",
        "\n",
        "class OptimizedTFIDFModel:\n",
        "    def __init__(self, corpus_df):\n",
        "        self.corpus = corpus_df\n",
        "        self.doc_ids = corpus_df['doc_id'].tolist()\n",
        "        self.documents = [' '.join(doc) for doc in corpus_df['processed_text']]\n",
        "        \n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            lowercase=False,\n",
        "            token_pattern=r'\\S+',\n",
        "            max_features=50000,\n",
        "            min_df=2,\n",
        "            max_df=0.95,\n",
        "            sublinear_tf=True,\n",
        "            norm='l2'\n",
        "        )\n",
        "        \n",
        "        print(\"Fitting TF-IDF vectorizer...\")\n",
        "        self.tfidf_matrix = self.vectorizer.fit_transform(self.documents)\n",
        "        print(f\"TF-IDF matrix shape: {self.tfidf_matrix.shape}\")\n",
        "        \n",
        "        self.doc_id_to_idx = {doc_id: idx for idx, doc_id in enumerate(self.doc_ids)}\n",
        "    \n",
        "    def rank_documents(self, query_terms, top_n=100):\n",
        "        query_text = ' '.join(query_terms)\n",
        "        query_vector = self.vectorizer.transform([query_text])\n",
        "        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
        "        \n",
        "        top_indices = np.argsort(similarities)[::-1][:top_n]\n",
        "        \n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            if similarities[idx] > 0:\n",
        "                doc_id = self.doc_ids[idx]\n",
        "                score = similarities[idx]\n",
        "                results.append((doc_id, score))\n",
        "        \n",
        "        return results\n",
        "\n",
        "class BM25Model:\n",
        "    def __init__(self, corpus_df, k1=1.2, b=0.75):\n",
        "        self.corpus = corpus_df\n",
        "        self.doc_ids = corpus_df['doc_id'].tolist()\n",
        "        self.tokenized_corpus = corpus_df['processed_text'].tolist()\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        \n",
        "        self.bm25 = BM25Okapi(self.tokenized_corpus, k1=k1, b=b)\n",
        "        self.avg_doc_length = np.mean([len(doc) for doc in self.tokenized_corpus])\n",
        "    \n",
        "    def rank_documents(self, query_terms, top_n=100):\n",
        "        doc_scores = self.bm25.get_scores(query_terms)\n",
        "        top_indices = np.argsort(doc_scores)[::-1][:top_n]\n",
        "        \n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            if doc_scores[idx] > 0:\n",
        "                doc_id = self.doc_ids[idx]\n",
        "                score = doc_scores[idx]\n",
        "                results.append((doc_id, score))\n",
        "        \n",
        "        return results\n",
        "\n",
        "print(\"Classical IR model classes defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Neural IR Model\n",
        "\n",
        "class NeuralIRModel:\n",
        "    def __init__(self, corpus_df, device='cpu'):\n",
        "        self.corpus = corpus_df\n",
        "        self.doc_ids = corpus_df['doc_id'].tolist()\n",
        "        self.device = device\n",
        "        \n",
        "        # Initialize bi-encoder\n",
        "        self.bi_encoder = SentenceTransformer('msmarco-distilbert-base-v4', device=device)\n",
        "        self.embedding_dim = self.bi_encoder.get_sentence_embedding_dimension()\n",
        "        \n",
        "        # Initialize cross-encoder\n",
        "        self.cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device=device)\n",
        "        \n",
        "        # Load or create embeddings\n",
        "        self.embeddings_file = 'corpus_embeddings.npy'\n",
        "        if os.path.exists(self.embeddings_file):\n",
        "            print(f\"Loading pre-computed embeddings from {self.embeddings_file}...\")\n",
        "            self.corpus_embeddings = np.load(self.embeddings_file)\n",
        "        else:\n",
        "            print(\"Encoding corpus...\")\n",
        "            corpus_texts = corpus_df['text'].tolist()\n",
        "            self.corpus_embeddings = self.bi_encoder.encode(\n",
        "                corpus_texts, show_progress_bar=True, batch_size=256, convert_to_tensor=True\n",
        "            ).cpu().numpy()\n",
        "            np.save(self.embeddings_file, self.corpus_embeddings)\n",
        "        \n",
        "        # Build FAISS index\n",
        "        self.index = faiss.IndexFlatL2(self.embedding_dim)\n",
        "        self.index.add(self.corpus_embeddings)\n",
        "        \n",
        "        print(f\"Neural IR model initialized. Index size: {self.index.ntotal}\")\n",
        "    \n",
        "    def search_and_rerank(self, query_text, top_k_retrieval=100, top_k_rerank=50):\n",
        "        # Stage 1: Bi-encoder retrieval\n",
        "        query_embedding = self.bi_encoder.encode(query_text, convert_to_tensor=True).cpu().numpy().reshape(1, -1)\n",
        "        distances, indices = self.index.search(query_embedding, top_k_retrieval)\n",
        "        \n",
        "        retrieved_doc_ids = [self.doc_ids[i] for i in indices[0]]\n",
        "        \n",
        "        # Stage 2: Cross-encoder re-ranking\n",
        "        cross_encoder_input = [[query_text, doc_id_to_text[doc_id]] for doc_id in retrieved_doc_ids]\n",
        "        cross_scores = self.cross_encoder.predict(cross_encoder_input, show_progress_bar=False)\n",
        "        \n",
        "        reranked_results = list(zip(retrieved_doc_ids, cross_scores))\n",
        "        reranked_results.sort(key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        return reranked_results[:top_k_rerank]\n",
        "\n",
        "print(\"Neural IR model class defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Initialize All Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check device availability\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize all models\n",
        "print(\"\\n=== Initializing Classical IR Models ===\")\n",
        "tfidf_model = OptimizedTFIDFModel(corpus_df)\n",
        "bm25_default = BM25Model(corpus_df, k1=1.2, b=0.75)\n",
        "bm25_optimized = BM25Model(corpus_df, k1=0.8, b=0.6)\n",
        "\n",
        "print(\"\\n=== Initializing Neural IR Model ===\")\n",
        "neural_model = NeuralIRModel(corpus_df, device=device)\n",
        "\n",
        "print(\"\\nAll models initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance Evaluation Functions\n",
        "\n",
        "def precision_at_k(retrieved, relevant, k):\n",
        "    \"\"\"Calculate Precision@k\"\"\"\n",
        "    retrieved_k = retrieved[:k]\n",
        "    if k == 0:\n",
        "        return 0.0\n",
        "    return len(set(retrieved_k) & set(relevant)) / k\n",
        "\n",
        "def recall_at_k(retrieved, relevant, k):\n",
        "    \"\"\"Calculate Recall@k\"\"\"\n",
        "    retrieved_k = retrieved[:k]\n",
        "    if not relevant:\n",
        "        return 0.0\n",
        "    return len(set(retrieved_k) & set(relevant)) / len(relevant)\n",
        "\n",
        "def average_precision(retrieved, relevant):\n",
        "    \"\"\"Calculate Average Precision (AP)\"\"\"\n",
        "    if not relevant:\n",
        "        return 0.0\n",
        "    \n",
        "    hits = 0\n",
        "    sum_precisions = 0.0\n",
        "    for i, doc_id in enumerate(retrieved):\n",
        "        if doc_id in relevant:\n",
        "            hits += 1\n",
        "            sum_precisions += hits / (i + 1)\n",
        "    \n",
        "    return sum_precisions / len(relevant)\n",
        "\n",
        "def mean_average_precision(results, relevant_docs):\n",
        "    \"\"\"Calculate Mean Average Precision (MAP)\"\"\"\n",
        "    aps = [average_precision(results[qid], relevant_docs.get(qid, [])) for qid in results]\n",
        "    return np.mean(aps)\n",
        "\n",
        "def mean_reciprocal_rank(results, relevant_docs):\n",
        "    \"\"\"Calculate Mean Reciprocal Rank (MRR)\"\"\"\n",
        "    rrs = []\n",
        "    for qid in results:\n",
        "        relevant = relevant_docs.get(qid, [])\n",
        "        for i, doc_id in enumerate(results[qid]):\n",
        "            if doc_id in relevant:\n",
        "                rrs.append(1 / (i + 1))\n",
        "                break\n",
        "        else:\n",
        "            rrs.append(0.0)\n",
        "    return np.mean(rrs)\n",
        "\n",
        "def ndcg_at_k(retrieved, relevant, k):\n",
        "    \"\"\"Calculate Normalized Discounted Cumulative Gain@k\"\"\"\n",
        "    retrieved_k = retrieved[:k]\n",
        "    dcg = 0.0\n",
        "    for i, doc_id in enumerate(retrieved_k):\n",
        "        if doc_id in relevant:\n",
        "            dcg += 1 / math.log2(i + 2)\n",
        "    \n",
        "    idcg = 0.0\n",
        "    num_relevant_k = min(len(relevant), k)\n",
        "    for i in range(num_relevant_k):\n",
        "        idcg += 1 / math.log2(i + 2)\n",
        "        \n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "def f1_at_k(retrieved, relevant, k):\n",
        "    \"\"\"Calculate F1@k\"\"\"\n",
        "    p = precision_at_k(retrieved, relevant, k)\n",
        "    r = recall_at_k(retrieved, relevant, k)\n",
        "    if p + r == 0:\n",
        "        return 0.0\n",
        "    return 2 * p * r / (p + r)\n",
        "\n",
        "def success_at_k(retrieved, relevant, k):\n",
        "    \"\"\"Success@k (a.k.a. HitRate@k): 1 if at least one relevant doc in top-k, else 0\"\"\"\n",
        "    retrieved_k = retrieved[:k]\n",
        "    if not relevant:\n",
        "        return 0.0\n",
        "    return 1.0 if any(doc_id in relevant for doc_id in retrieved_k) else 0.0\n",
        "\n",
        "def evaluate_model(results, relevant_docs, k_values=[1, 3, 5, 10, 20,100]):\n",
        "    \"\"\"Comprehensive evaluation of a retrieval model\"\"\"\n",
        "    metrics = {}\n",
        "    \n",
        "    for k in k_values:\n",
        "        precisions = [precision_at_k(results[qid], relevant_docs.get(qid, []), k) for qid in results]\n",
        "        recalls = [recall_at_k(results[qid], relevant_docs.get(qid, []), k) for qid in results]\n",
        "        ndcgs = [ndcg_at_k(results[qid], relevant_docs.get(qid, []), k) for qid in results]\n",
        "        f1s = [f1_at_k(results[qid], relevant_docs.get(qid, []), k) for qid in results]\n",
        "        successes  = [success_at_k(results[qid],   relevant_docs.get(qid, []), k) for qid in results]\n",
        "        \n",
        "        metrics[f'P@{k}'] = np.mean(precisions)\n",
        "        metrics[f'R@{k}'] = np.mean(recalls)\n",
        "        metrics[f'nDCG@{k}'] = np.mean(ndcgs)\n",
        "        metrics[f'F1@{k}'] = np.mean(f1s)\n",
        "        metrics[f'Success@{k}'] = np.mean(successes)\n",
        "    \n",
        "    metrics['MAP'] = mean_average_precision(results, relevant_docs)\n",
        "    metrics['MRR'] = mean_reciprocal_rank(results, relevant_docs)\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "print(\"Performance evaluation functions loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Efficiency Evaluation Functions\n",
        "\n",
        "def measure_memory_usage():\n",
        "    \"\"\"Get current memory usage in MB\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    return process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "def measure_cpu_usage():\n",
        "    \"\"\"Get current CPU usage percentage\"\"\"\n",
        "    return psutil.cpu_percent()\n",
        "\n",
        "def measure_query_latency(model_func, query, *args, **kwargs):\n",
        "    \"\"\"Measure the time taken to process a single query\"\"\"\n",
        "    start_time = time.time()\n",
        "    result = model_func(query, *args, **kwargs)\n",
        "    end_time = time.time()\n",
        "    return result, (end_time - start_time) * 1000  # Return latency in milliseconds\n",
        "\n",
        "def measure_throughput(model_func, queries, *args, **kwargs):\n",
        "    \"\"\"Measure queries processed per second\"\"\"\n",
        "    start_time = time.time()\n",
        "    for query in queries:\n",
        "        model_func(query, *args, **kwargs)\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    return len(queries) / total_time\n",
        "\n",
        "def get_model_size(model):\n",
        "    \"\"\"Estimate model size in MB\"\"\"\n",
        "    if hasattr(model, 'tfidf_matrix'):\n",
        "        # TF-IDF model\n",
        "        return model.tfidf_matrix.data.nbytes / 1024 / 1024\n",
        "    elif hasattr(model, 'bm25'):\n",
        "        # BM25 model\n",
        "        return sum(len(doc) for doc in model.tokenized_corpus) * 4 / 1024 / 1024  # Rough estimate\n",
        "    elif hasattr(model, 'corpus_embeddings'):\n",
        "        # Neural model\n",
        "        return model.corpus_embeddings.nbytes / 1024 / 1024\n",
        "    return 0\n",
        "\n",
        "def get_index_size(model):\n",
        "    \"\"\"Get index size in MB\"\"\"\n",
        "    if hasattr(model, 'tfidf_matrix'):\n",
        "        return model.tfidf_matrix.data.nbytes / 1024 / 1024\n",
        "    elif hasattr(model, 'index'):\n",
        "        return model.index.ntotal * model.embedding_dim * 4 / 1024 / 1024  # 4 bytes per float32\n",
        "    return 0\n",
        "\n",
        "print(\"Efficiency evaluation functions loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Run Comprehensive Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select evaluation queries\n",
        "rel_counts = qrels_df.groupby('query_id').size()\n",
        "rich_queries = rel_counts[rel_counts >= 5]  # At least 3 relevant documents\n",
        "eval_query_ids = rich_queries.index[:20].tolist()  # Evaluate on 50 queries\n",
        "\n",
        "print(f\"Evaluating on {len(eval_query_ids)} queries with at least 3 relevant documents\")\n",
        "print(f\"Average relevant documents per query: {rich_queries.mean():.2f}\")\n",
        "\n",
        "# Run retrieval with all models\n",
        "results_tfidf = {}\n",
        "results_bm25_default = {}\n",
        "results_bm25_optimized = {}\n",
        "results_neural = {}\n",
        "\n",
        "print(\"\\n=== Running Retrieval with All Models ===\")\n",
        "\n",
        "for qid in tqdm(eval_query_ids, desc=\"Processing Queries\"):\n",
        "    query_terms = query_id_to_text[qid]\n",
        "    query_original = query_id_to_original[qid]\n",
        "    \n",
        "    # TF-IDF results\n",
        "    tfidf_ranked_list = tfidf_model.rank_documents(query_terms, top_n=100)\n",
        "    results_tfidf[qid] = [doc_id for doc_id, score in tfidf_ranked_list]\n",
        "    \n",
        "    # BM25 results (default parameters)\n",
        "    bm25_default_ranked_list = bm25_default.rank_documents(query_terms, top_n=100)\n",
        "    results_bm25_default[qid] = [doc_id for doc_id, score in bm25_default_ranked_list]\n",
        "    \n",
        "    # BM25 results (optimized parameters)\n",
        "    bm25_optimized_ranked_list = bm25_optimized.rank_documents(query_terms, top_n=100)\n",
        "    results_bm25_optimized[qid] = [doc_id for doc_id, score in bm25_optimized_ranked_list]\n",
        "    \n",
        "    # Neural results\n",
        "    neural_ranked_list = neural_model.search_and_rerank(query_original, top_k_retrieval=100, top_k_rerank=100)\n",
        "    results_neural[qid] = [doc_id for doc_id, score in neural_ranked_list]\n",
        "\n",
        "print(\"\\nRetrieval completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate performance metrics for all models\n",
        "print(\"=== Calculating Performance Metrics ===\")\n",
        "\n",
        "tfidf_metrics = evaluate_model(results_tfidf, relevant_docs)\n",
        "bm25_default_metrics = evaluate_model(results_bm25_default, relevant_docs)\n",
        "bm25_optimized_metrics = evaluate_model(results_bm25_optimized, relevant_docs)\n",
        "neural_metrics = evaluate_model(results_neural, relevant_docs)\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_df = pd.DataFrame([\n",
        "    tfidf_metrics,\n",
        "    bm25_optimized_metrics,\n",
        "    neural_metrics\n",
        "], index=['TF-IDF', 'BM25', 'Neural (Bi+Cross)'])\n",
        "\n",
        "print(comparison_df)\n",
        "# Select only the requested metrics\n",
        "cols_to_show = [\"P@1\", \"P@5\", \"nDCG@10\",\"F1@10\", \"nDCG@5\" , \"R@100\", \"MAP\",\"MRR\",\"Success@1\"]\n",
        "\n",
        "print(\"\\n=== Performance Metrics Comparison ===\")\n",
        "display(comparison_df[cols_to_show].round(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Measure efficiency metrics\n",
        "print(\"=== Measuring Efficiency Metrics ===\")\n",
        "\n",
        "# Sample queries for efficiency testing\n",
        "sample_queries = [query_id_to_text[qid] for qid in eval_query_ids[:10]]\n",
        "sample_queries_original = [query_id_to_original[qid] for qid in eval_query_ids[:10]]\n",
        "\n",
        "# Measure query latency\n",
        "print(\"Measuring query latency...\")\n",
        "latency_results = {}\n",
        "\n",
        "# TF-IDF latency\n",
        "tfidf_latencies = []\n",
        "for query in sample_queries:\n",
        "    _, latency = measure_query_latency(tfidf_model.rank_documents, query, top_n=100)\n",
        "    tfidf_latencies.append(latency)\n",
        "latency_results['TF-IDF'] = np.mean(tfidf_latencies)\n",
        "\n",
        "\n",
        "# BM25 optimized latency\n",
        "bm25_optimized_latencies = []\n",
        "for query in sample_queries:\n",
        "    _, latency = measure_query_latency(bm25_optimized.rank_documents, query, top_n=100)\n",
        "    bm25_optimized_latencies.append(latency)\n",
        "latency_results['BM25'] = np.mean(bm25_optimized_latencies)\n",
        "\n",
        "# Neural latency\n",
        "neural_latencies = []\n",
        "for query in sample_queries_original:\n",
        "    _, latency = measure_query_latency(neural_model.search_and_rerank, query, top_k_retrieval=100, top_k_rerank=100)\n",
        "    neural_latencies.append(latency)\n",
        "latency_results['Neural (Bi+Cross)'] = np.mean(neural_latencies)\n",
        "\n",
        "print(\"Query latency measurements completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Measure throughput\n",
        "print(\"Measuring throughput...\")\n",
        "throughput_results = {}\n",
        "\n",
        "throughput_results['TF-IDF'] = measure_throughput(tfidf_model.rank_documents, sample_queries, top_n=100)\n",
        "throughput_results['BM25'] = measure_throughput(bm25_optimized.rank_documents, sample_queries, top_n=100)\n",
        "throughput_results['Neural (Bi+Cross)'] = measure_throughput(\n",
        "    neural_model.search_and_rerank, \n",
        "    sample_queries_original, \n",
        "    top_k_retrieval=100, \n",
        "    top_k_rerank=100\n",
        ")\n",
        "print(\"Throughput measurements completed!\")\n",
        "\n",
        "# Measure model and index sizes\n",
        "print(\"Measuring model and index sizes...\")\n",
        "model_sizes = {}\n",
        "index_sizes = {}\n",
        "\n",
        "model_sizes['TF-IDF'] = get_model_size(tfidf_model)\n",
        "model_sizes['BM25'] = get_model_size(bm25_optimized)   # renamed here\n",
        "model_sizes['Neural (Bi+Cross)'] = get_model_size(neural_model)\n",
        "\n",
        "index_sizes['TF-IDF'] = get_index_size(tfidf_model)\n",
        "index_sizes['BM25'] = get_index_size(bm25_optimized)   # renamed here\n",
        "index_sizes['Neural (Bi+Cross)'] = get_index_size(neural_model)\n",
        "print(\"Size measurements completed!\")\n",
        "\n",
        "# Create efficiency comparison DataFrame\n",
        "efficiency_df = pd.DataFrame({\n",
        "    'Query Latency (ms)': latency_results,\n",
        "    'Throughput (qps)': throughput_results,\n",
        "    'Model Size (MB)': model_sizes,\n",
        "    'Index Size (MB)': index_sizes\n",
        "})\n",
        "\n",
        "print(\"\\n=== Efficiency Metrics Comparison ===\")\n",
        "display(efficiency_df.round(2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Comprehensive Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance Metrics Visualization\n",
        "def plot_performance_comparison(comparison_df):\n",
        "    \"\"\"Create comprehensive performance comparison visualizations\"\"\"\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    \n",
        "    # MAP and MRR comparison\n",
        "    map_mrr_data = comparison_df[['MAP', 'MRR']]\n",
        "    map_mrr_data.plot(kind='bar', ax=axes[0, 0], color=['skyblue', 'lightcoral'])\n",
        "    axes[0, 0].set_title('MAP and MRR Comparison', fontweight='bold', fontsize=14)\n",
        "    axes[0, 0].set_ylabel('Score')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Precision@k comparison\n",
        "    p_at_k_cols = [col for col in comparison_df.columns if col.startswith('P@')]\n",
        "    comparison_df[p_at_k_cols].plot(kind='bar', ax=axes[0, 1], width=0.8)\n",
        "    axes[0, 1].set_title('Precision@k Comparison', fontweight='bold', fontsize=14)\n",
        "    axes[0, 1].set_ylabel('Precision')\n",
        "    axes[0, 1].legend(title='k values')\n",
        "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # nDCG@k comparison\n",
        "    ndcg_at_k_cols = [col for col in comparison_df.columns if col.startswith('nDCG@')]\n",
        "    comparison_df[ndcg_at_k_cols].plot(kind='bar', ax=axes[0, 2], width=0.8)\n",
        "    axes[0, 2].set_title('nDCG@k Comparison', fontweight='bold', fontsize=14)\n",
        "    axes[0, 2].set_ylabel('nDCG')\n",
        "    axes[0, 2].legend(title='k values')\n",
        "    axes[0, 2].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # F1@k comparison\n",
        "    f1_at_k_cols = [col for col in comparison_df.columns if col.startswith('F1@')]\n",
        "    comparison_df[f1_at_k_cols].plot(kind='bar', ax=axes[1, 0], width=0.8)\n",
        "    axes[1, 0].set_title('F1@k Comparison', fontweight='bold', fontsize=14)\n",
        "    axes[1, 0].set_ylabel('F1 Score')\n",
        "    axes[1, 0].legend(title='k values')\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Recall@k comparison\n",
        "    r_at_k_cols = [col for col in comparison_df.columns if col.startswith('R@')]\n",
        "    comparison_df[r_at_k_cols].plot(kind='bar', ax=axes[1, 1], width=0.8)\n",
        "    axes[1, 1].set_title('Recall@k Comparison', fontweight='bold', fontsize=14)\n",
        "    axes[1, 1].set_ylabel('Recall')\n",
        "    axes[1, 1].legend(title='k values')\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Performance improvement analysis\n",
        "    improvements = []\n",
        "    models = ['TF-IDF', 'BM25', 'Neural (Bi+Cross)']\n",
        "    baseline_map = comparison_df.loc['TF-IDF', 'MAP']\n",
        "\n",
        "    for model in models:\n",
        "        if model != 'TF-IDF':\n",
        "            improvement = ((comparison_df.loc[model, 'MAP'] - baseline_map) / baseline_map * 100)\n",
        "            improvements.append(improvement)\n",
        "        else:\n",
        "            improvements.append(0)\n",
        "\n",
        "    axes[1, 2].plot(models, improvements, marker='o', linestyle='-', color='darkgreen')\n",
        "    axes[1, 2].set_title('MAP Improvement over TF-IDF', fontweight='bold', fontsize=14)\n",
        "    axes[1, 2].set_ylabel('Improvement (%)')\n",
        "    axes[1, 2].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Create performance visualizations\n",
        "plot_performance_comparison(comparison_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Efficiency Metrics Visualization\n",
        "def plot_efficiency_comparison(efficiency_df):\n",
        "    \"\"\"Create comprehensive efficiency comparison visualizations\"\"\"\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # Query Latency comparison\n",
        "    efficiency_df['Query Latency (ms)'].plot(kind='bar', ax=axes[0, 0], color='lightcoral')\n",
        "    axes[0, 0].set_title('Query Latency Comparison', fontweight='bold', fontsize=14)\n",
        "    axes[0, 0].set_ylabel('Latency (milliseconds)')\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Throughput comparison\n",
        "    efficiency_df['Throughput (qps)'].plot(kind='bar', ax=axes[0, 1], color='lightgreen')\n",
        "    axes[0, 1].set_title('Throughput Comparison', fontweight='bold', fontsize=14)\n",
        "    axes[0, 1].set_ylabel('Queries per Second')\n",
        "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Model Size comparison\n",
        "    efficiency_df['Model Size (MB)'].plot(kind='bar', ax=axes[1, 0], color='lightblue')\n",
        "    axes[1, 0].set_title('Model Size Comparison', fontweight='bold', fontsize=14)\n",
        "    axes[1, 0].set_ylabel('Size (MB)')\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Index Size comparison\n",
        "    efficiency_df['Index Size (MB)'].plot(kind='bar', ax=axes[1, 1], color='lightyellow')\n",
        "    axes[1, 1].set_title('Index Size Comparison', fontweight='bold', fontsize=14)\n",
        "    axes[1, 1].set_ylabel('Size (MB)')\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create efficiency visualizations\n",
        "plot_efficiency_comparison(efficiency_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance vs Efficiency Trade-off Analysis\n",
        "def plot_performance_efficiency_tradeoff(comparison_df, efficiency_df):\n",
        "    \"\"\"Create performance vs efficiency trade-off analysis\"\"\"\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # MAP vs Query Latency\n",
        "    axes[0].scatter(efficiency_df['Query Latency (ms)'], comparison_df['MAP'], \n",
        "                   s=200, alpha=0.7, c=['red', 'blue', 'green', 'orange'])\n",
        "    \n",
        "    for i, model in enumerate(comparison_df.index):\n",
        "        axes[0].annotate(model, \n",
        "                        (efficiency_df['Query Latency (ms)'][i], comparison_df['MAP'][i]),\n",
        "                        xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
        "    \n",
        "    axes[0].set_xlabel('Query Latency (ms)')\n",
        "    axes[0].set_ylabel('MAP Score')\n",
        "    axes[0].set_title('Performance vs Query Latency Trade-off', fontweight='bold', fontsize=14)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # MAP vs Model Size\n",
        "    axes[1].scatter(efficiency_df['Model Size (MB)'], comparison_df['MAP'], \n",
        "                   s=200, alpha=0.7, c=['red', 'blue', 'green', 'orange'])\n",
        "    \n",
        "    for i, model in enumerate(comparison_df.index):\n",
        "        axes[1].annotate(model, \n",
        "                        (efficiency_df['Model Size (MB)'][i], comparison_df['MAP'][i]),\n",
        "                        xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
        "    \n",
        "    axes[1].set_xlabel('Model Size (MB)')\n",
        "    axes[1].set_ylabel('MAP Score')\n",
        "    axes[1].set_title('Performance vs Model Size Trade-off', fontweight='bold', fontsize=14)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create trade-off analysis\n",
        "plot_performance_efficiency_tradeoff(comparison_df, efficiency_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Comprehensive Analysis and Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance Analysis\n",
        "print(\"=== PERFORMANCE ANALYSIS ===\")\n",
        "print(f\"Dataset: {len(corpus_df)} documents, {len(queries_df)} queries\")\n",
        "print(f\"Evaluation: {len(eval_query_ids)} queries with sufficient relevance judgments\")\n",
        "print(f\"Average relevant documents per query: {rich_queries.mean():.2f}\")\n",
        "\n",
        "print(\"\\n=== Model Performance Ranking (by MAP) ===\")\n",
        "sorted_models = comparison_df.sort_values('MAP', ascending=False)\n",
        "for i, (model, row) in enumerate(sorted_models.iterrows(), 1):\n",
        "    print(f\"{i}. {model}: MAP = {row['MAP']:.4f}, MRR = {row['MRR']:.4f}\")\n",
        "\n",
        "print(\"\\n=== Performance Improvements over TF-IDF ===\")\n",
        "baseline_map = comparison_df.loc['TF-IDF', 'MAP']\n",
        "baseline_mrr = comparison_df.loc['TF-IDF', 'MRR']\n",
        "\n",
        "for model in ['BM25 (Default)', 'BM25 (Optimized)', 'Neural (Bi+Cross)']:\n",
        "    map_improvement = ((comparison_df.loc[model, 'MAP'] - baseline_map) / baseline_map * 100)\n",
        "    mrr_improvement = ((comparison_df.loc[model, 'MRR'] - baseline_mrr) / baseline_mrr * 100)\n",
        "    print(f\"{model}:\")\n",
        "    print(f\"  MAP improvement: {map_improvement:.2f}%\")\n",
        "    print(f\"  MRR improvement: {mrr_improvement:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
